{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1015d1ba8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pandas'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"pandas\", \"I like pandas\"])\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4])\n",
    "squared = nums.map(lambda x: x*x).collect()\n",
    "for num in squared:\n",
    "    print (\"%i \" % (num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"hello world\",\"hi\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"hello world\",\"hi\"])\n",
    "words = lines.map(lambda line: line.split(\" \"))\n",
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'* The Python examples require urllib3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"README.md\")\n",
    "pythonLines = lines.filter(lambda line: \"Python\" in line)\n",
    "pythonLines.persist()\n",
    "pythonLines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonLines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCN,REPORTDATETIME,SHIFT,OFFENSE,METHOD,LASTMODIFIEDDATE,BLOCKSITEADDRESS,BLOCKXCOORD,BLOCKYCOORD,WARD,ANC,DISTRICT,PSA,NEIGHBORHOODCLUSTER,BUSINESSIMPROVEMENTDISTRICT,BLOCK_GROUP,CENSUS_TRACT,VOTING_PRECINCT,START_DATE,END_DATE',\n",
       " '04104147,4/16/2013 12:00:00 AM,MIDNIGHT,HOMICIDE,KNIFE,8/7/2015 8:34:01 AM,1500 - 1599 BLOCK OF 1ST STREET SW,398943,133729,6,6D,FIRST,105,9,,006400 2,006400,Precinct 127,7/27/2004 8:30:00 PM,7/27/2004 8:30:00 PM',\n",
       " '05047867,6/5/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,KNIFE,8/7/2015 8:32:22 AM,6500  - 6599 BLOCK OF PINEY BRANCH ROAD NW,397769,144596,4,4B,FOURTH,402,17,,001901 4,001901,Precinct 59,4/15/2005 12:30:00 PM,',\n",
       " '07083463,7/8/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:32:15 AM,1800 - 1810 BLOCK OF COLUMBIA ROAD NW,396275,139402,1,1C,THIRD,303,1,ADAMS MORGAN,004002 1,004002,Precinct 25,7/14/2007 3:00:00 PM,',\n",
       " '09172197,4/8/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:33:35 AM,2322 - 2499 BLOCK OF ONTARIO ROAD NW,396518,139335,1,1C,THIRD,303,1,,003800 1,003800,Precinct 24,5/22/2009 1:00:00 PM,5/22/2009 3:00:00 AM']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_rdd = sc.textFile(\"/Users/tim.wu/Documents/python_code/python_spark_basics/Spark-Essentials/Datasets/washington-dc-crime/wash_dc_crime_incidents_2013.csv\")\n",
    "base_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04104147,4/16/2013 12:00:00 AM,MIDNIGHT,HOMICIDE,KNIFE,8/7/2015 8:34:01 AM,1500 - 1599 BLOCK OF 1ST STREET SW,398943,133729,6,6D,FIRST,105,9,,006400 2,006400,Precinct 127,7/27/2004 8:30:00 PM,7/27/2004 8:30:00 PM',\n",
       " '05047867,6/5/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,KNIFE,8/7/2015 8:32:22 AM,6500  - 6599 BLOCK OF PINEY BRANCH ROAD NW,397769,144596,4,4B,FOURTH,402,17,,001901 4,001901,Precinct 59,4/15/2005 12:30:00 PM,',\n",
       " '07083463,7/8/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:32:15 AM,1800 - 1810 BLOCK OF COLUMBIA ROAD NW,396275,139402,1,1C,THIRD,303,1,ADAMS MORGAN,004002 1,004002,Precinct 25,7/14/2007 3:00:00 PM,',\n",
       " '09172197,4/8/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:33:35 AM,2322 - 2499 BLOCK OF ONTARIO ROAD NW,396518,139335,1,1C,THIRD,303,1,,003800 1,003800,Precinct 24,5/22/2009 1:00:00 PM,5/22/2009 3:00:00 AM',\n",
       " '09251354,2/27/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:32:39 AM,2500 - 2699 BLOCK OF VIRGINIA AVENUE NW,395232,136881,2,2A,SECOND,207,5,,005600 2,005600,Precinct 3,1/1/2009 2:05:00 PM,1/1/2009 3:00:00 PM',\n",
       " '10028985,2/27/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:33:47 AM,1800 - 2299 BLOCK OF NEW YORK AVENUE NE,402158.31,138824.53,5,5C,FIFTH,505,22,,011100 3,011100,Precinct 72,3/7/2010 2:30:00 AM,3/7/2010 2:45:00 AM',\n",
       " '10033521,10/10/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:33:44 AM,2800 - 2821 BLOCK OF PENNSYLVANIA AVENUE SE,402837,133810,7,7B,SIXTH,607,34,,007604 3,007604,Precinct 111,12/29/2008 8:00:00 PM,12/29/2008 8:00:00 PM',\n",
       " '10124918,4/9/2013 12:00:00 AM,MIDNIGHT,SEX ABUSE,OTHERS,8/7/2015 8:32:09 AM,1 - 99 BLOCK OF DC VILLAGE LANE SW,398794,127300,8,8D,SEVENTH,708,,,010900 2,010900,Precinct 126,8/27/2010 5:00:00 PM,',\n",
       " '11010107,7/31/2013 12:00:00 AM,MIDNIGHT,HOMICIDE,OTHERS,8/7/2015 8:33:34 AM,3000 - 3099 BLOCK OF 7TH STREET NE,400331,140004,5,5E,FIFTH,502,21,,009201 1,009201,Precinct 74,1/23/2011 7:50:00 AM,',\n",
       " '11045512,1/31/2013 12:00:00 AM,MIDNIGHT,HOMICIDE,GUN,8/7/2015 8:33:47 AM,400 - 499 BLOCK OF BURBANK STREET SE,404939,135095,7,7F,SIXTH,603,33,,007703 2,007703,Precinct 103,4/5/2011 10:30:00 PM,4/5/2011 10:32:00 PM']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_header_rdd = base_rdd.filter(lambda line: 'REPORTDATETIME' not in line)\n",
    "no_header_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CrimeData(ccn='04104147', report_time='4/16/2013 12:00:00 AM', shift='MIDNIGHT', offense='HOMICIDE', method='KNIFE'),\n",
      " CrimeData(ccn='05047867', report_time='6/5/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='KNIFE'),\n",
      " CrimeData(ccn='07083463', report_time='7/8/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='OTHERS'),\n",
      " CrimeData(ccn='09172197', report_time='4/8/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='OTHERS'),\n",
      " CrimeData(ccn='09251354', report_time='2/27/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='OTHERS'),\n",
      " CrimeData(ccn='10028985', report_time='2/27/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='OTHERS'),\n",
      " CrimeData(ccn='10033521', report_time='10/10/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='OTHERS'),\n",
      " CrimeData(ccn='10124918', report_time='4/9/2013 12:00:00 AM', shift='MIDNIGHT', offense='SEX ABUSE', method='OTHERS'),\n",
      " CrimeData(ccn='11010107', report_time='7/31/2013 12:00:00 AM', shift='MIDNIGHT', offense='HOMICIDE', method='OTHERS'),\n",
      " CrimeData(ccn='11045512', report_time='1/31/2013 12:00:00 AM', shift='MIDNIGHT', offense='HOMICIDE', method='GUN')]\n"
     ]
    }
   ],
   "source": [
    "CrimeData = namedtuple('CrimeData', ['ccn', 'report_time', 'shift', 'offense', 'method'])\n",
    "\n",
    "def map_line(line):\n",
    "  columns = line.split(\",\")[:5]\n",
    "  return CrimeData(*columns)\n",
    "\n",
    "data_rdd = no_header_rdd.map(map_line)\n",
    "pprint(data_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('HOMICIDE', <pyspark.resultiterable.ResultIterable object at 0x10c6a7b70>), ('ASSAULT W/DANGEROUS WEAPON', <pyspark.resultiterable.ResultIterable object at 0x10c6a7898>), ('ROBBERY', <pyspark.resultiterable.ResultIterable object at 0x10c947ba8>), ('BURGLARY', <pyspark.resultiterable.ResultIterable object at 0x10c6a7e48>), ('THEFT F/AUTO', <pyspark.resultiterable.ResultIterable object at 0x10c6a7dd8>), ('MOTOR VEHICLE THEFT', <pyspark.resultiterable.ResultIterable object at 0x10c6ac320>), ('ARSON', <pyspark.resultiterable.ResultIterable object at 0x10c6ac5c0>), ('THEFT/OTHER', <pyspark.resultiterable.ResultIterable object at 0x10c6ac668>), ('SEX ABUSE', <pyspark.resultiterable.ResultIterable object at 0x10da0fcf8>)]\n"
     ]
    }
   ],
   "source": [
    "grouped_by_offense_rdd = data_rdd.groupBy(lambda data: data.offense)\n",
    "\n",
    "# What does this return? You'll need to know for the next step.\n",
    "print (grouped_by_offense_rdd.take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOMICIDE                       104\n",
      "ASSAULT W/DANGEROUS WEAPON     2309\n",
      "ROBBERY                        4071\n",
      "BURGLARY                       3370\n",
      "THEFT F/AUTO                   10130\n",
      "MOTOR VEHICLE THEFT            2675\n",
      "ARSON                          35\n",
      "THEFT/OTHER                    12904\n",
      "SEX ABUSE                      299\n"
     ]
    }
   ],
   "source": [
    "offense_counts = grouped_by_offense_rdd.map(lambda g: (g[0], len(g[1]))).collect()\n",
    "for offense, count in offense_counts:\n",
    "  print (\"{0:30s} {1:d}\".format(offense, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSAULT W/DANGEROUS WEAPON     2309\n",
      "ROBBERY                        4071\n",
      "THEFT F/AUTO                   10130\n",
      "SEX ABUSE                      299\n",
      "MOTOR VEHICLE THEFT            2675\n",
      "BURGLARY                       3370\n",
      "THEFT/OTHER                    12904\n",
      "HOMICIDE                       104\n",
      "ARSON                          35\n"
     ]
    }
   ],
   "source": [
    "offense_counts = data_rdd.map(lambda item: (item.offense, item)).countByKey()\n",
    "for offense, counts in offense_counts.items():\n",
    "  print (\"{0:30s} {1:d}\".format(offense, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ARSON': <pyspark.resultiterable.ResultIterable at 0x10c5f5128>,\n",
       " 'ASSAULT W/DANGEROUS WEAPON': <pyspark.resultiterable.ResultIterable at 0x10c542ba8>,\n",
       " 'BURGLARY': <pyspark.resultiterable.ResultIterable at 0x10c5422b0>,\n",
       " 'HOMICIDE': <pyspark.resultiterable.ResultIterable at 0x10c6c7748>,\n",
       " 'MOTOR VEHICLE THEFT': <pyspark.resultiterable.ResultIterable at 0x10c5f50f0>,\n",
       " 'ROBBERY': <pyspark.resultiterable.ResultIterable at 0x10c720b38>,\n",
       " 'SEX ABUSE': <pyspark.resultiterable.ResultIterable at 0x10db4ba20>,\n",
       " 'THEFT F/AUTO': <pyspark.resultiterable.ResultIterable at 0x10c542320>,\n",
       " 'THEFT/OTHER': <pyspark.resultiterable.ResultIterable at 0x10ca6c048>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_offense_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# How many partitions does the base RDD have? What about the `grouped_by_offense` RDD? How can you find out?\n",
    "print (base_rdd.getNumPartitions())\n",
    "print (grouped_by_offense_rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of lines: 35898\n",
      "Count of characters (Unicode): 7906305\n"
     ]
    }
   ],
   "source": [
    "print (\"Count of lines: {0}\".format(base_rdd.count()))\n",
    "totalChars = base_rdd.map(lambda line: len(line)).reduce(lambda a, b: a + b)\n",
    "print (\"Count of characters (Unicode): {0}\".format(totalChars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[50] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_rdd.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNIFE      11\n",
      "GUN        81\n",
      "OTHERS     12\n"
     ]
    }
   ],
   "source": [
    "result_rdd1 = data_rdd.filter(lambda item: item.offense == 'HOMICIDE')\\\n",
    "                      .map(lambda item: (item.method, 1))\\\n",
    "                      .reduceByKey(lambda i, j: i + j)\n",
    "\n",
    "for method, count in result_rdd1.collect():\n",
    "  print (\"{0:10s} {1:d}\".format(method, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('EVENING', 15167)\n"
     ]
    }
   ],
   "source": [
    "print (data_rdd.map(lambda item: (item.shift, 1))\n",
    "               .reduceByKey(lambda c1, c2: c1 + c2)\n",
    "               .max(key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4/16/2013 12:00:00 AM',\n",
       " '6/5/2013 12:00:00 AM',\n",
       " '7/8/2013 12:00:00 AM',\n",
       " '4/8/2013 12:00:00 AM',\n",
       " '2/27/2013 12:00:00 AM',\n",
       " '2/27/2013 12:00:00 AM',\n",
       " '10/10/2013 12:00:00 AM',\n",
       " '4/9/2013 12:00:00 AM',\n",
       " '7/31/2013 12:00:00 AM',\n",
       " '1/31/2013 12:00:00 AM',\n",
       " '7/8/2013 12:00:00 AM',\n",
       " '1/9/2013 12:59:00 AM',\n",
       " '3/23/2013 10:00:00 AM',\n",
       " '8/19/2013 12:00:00 AM',\n",
       " '5/13/2013 12:00:00 AM',\n",
       " '9/10/2013 6:43:00 PM',\n",
       " '11/13/2013 12:00:00 AM',\n",
       " '8/26/2013 12:00:00 AM',\n",
       " '1/1/2013 12:15:00 AM',\n",
       " '1/1/2013 12:50:00 AM',\n",
       " '1/1/2013 12:18:00 AM',\n",
       " '1/1/2013 2:10:00 AM',\n",
       " '1/1/2013 1:28:00 AM',\n",
       " '1/1/2013 3:00:00 AM',\n",
       " '1/1/2013 2:29:00 AM',\n",
       " '1/1/2013 2:24:00 AM',\n",
       " '1/1/2013 3:10:00 AM',\n",
       " '1/1/2013 3:12:00 AM',\n",
       " '1/1/2013 3:31:00 AM',\n",
       " '1/1/2013 3:45:00 AM']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md ### Demonstration\n",
    "# MAGIC \n",
    "# MAGIC Let's plot murders by month. DataFrames are useful for this one.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md To do this properly, we'll need to parse the dates. That will require knowing their format. A quick sampling of the data will help.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "data_rdd.map(lambda item: item.report_time).take(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Okay. We can now create a [`strptime()` format string](https://docs.python.org/2.7/library/datetime.html?highlight=strptime#datetime.datetime.strptime), allowing us to use the `datetime.datetime` Python class to parse those strings into actual datetime values.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from datetime import datetime\n",
    "date_format = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MAGIC %md Now, we can create the data frame. We'll start with the `no_header_rdd` and map it slightly differently than we did to create `data_rdd`:\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "def make_row(line):\n",
    "  columns = line.split(\",\")[:5]\n",
    "  columns[1] = datetime.strptime(columns[1], date_format)\n",
    "  return CrimeData(\n",
    "    columns[0],\n",
    "    columns[1],\n",
    "    columns[2],\n",
    "    columns[3],\n",
    "    columns[4])\n",
    "\n",
    "df = no_header_rdd.map(make_row).toDF()\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md Let's use a user-defined function (something supported by DataFrames and Spark SQL) to give us a `month` function we can use to extract just the month part of a `Timestamp`.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "month = udf(lambda dt: dt.month)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display( \n",
    "  df.filter(df['offense'] == 'HOMICIDE')\n",
    "    .select(month(df['report_time']).alias(\"month\"), df['offense'])\n",
    "    .groupBy('month').count()\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md What about all crimes per month?\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(\n",
    "  df.select(month(df['report_time']).alias('month'))\n",
    "    .groupBy('month').count()\n",
    ")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md We can also plot the frequency of crimes by hour of day.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "hour = udf(lambda dt: dt.hour)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(\n",
    "  df.select(hour(df[\"report_time\"]).alias(\"hour\"), df[\"offense\"]).groupBy(\"hour\").count()\n",
    ")\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
